{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "362dc2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Directory with training horse pictures\n",
    "train_horse_dir = os.path.join('./horse-or-human/horses')\n",
    "\n",
    "# Directory with training human pictures\n",
    "train_human_dir = os.path.join('./horse-or-human/humans')\n",
    "\n",
    "# Directory with validation horse pictures\n",
    "validation_horse_dir = os.path.join('./validation-horse-or-human/horses')\n",
    "\n",
    "# Directory with validation human pictures\n",
    "validation_human_dir = os.path.join('./validation-horse-or-human/humans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3549b39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SET HORSES: ['horse04-3.png', 'horse35-3.png', 'horse46-6.png', 'horse01-9.png', 'horse11-6.png', 'horse41-6.png', 'horse43-5.png', 'horse37-5.png', 'horse25-0.png', 'horse16-2.png']\n",
      "TRAIN SET HUMANS: ['human10-13.png', 'human08-12.png', 'human05-05.png', 'human15-26.png', 'human02-22.png', 'human08-30.png', 'human11-30.png', 'human16-01.png', 'human08-24.png', 'human01-25.png']\n",
      "VAL SET HORSES: ['horse1-224.png', 'horse1-484.png', 'horse4-556.png', 'horse3-484.png', 'horse2-201.png', 'horse4-501.png', 'horse4-389.png', 'horse2-294.png', 'horse5-076.png', 'horse4-541.png']\n",
      "VAL SET HUMANS: ['valhuman02-02.png', 'valhuman03-17.png', 'valhuman01-12.png', 'valhuman04-24.png', 'valhuman04-20.png', 'valhuman01-23.png', 'valhuman03-02.png', 'valhuman05-09.png', 'valhuman04-14.png', 'valhuman04-16.png']\n"
     ]
    }
   ],
   "source": [
    "train_horse_names = os.listdir(train_horse_dir)\n",
    "print(f'TRAIN SET HORSES: {train_horse_names[:10]}')\n",
    "\n",
    "train_human_names = os.listdir(train_human_dir)\n",
    "print(f'TRAIN SET HUMANS: {train_human_names[:10]}')\n",
    "\n",
    "validation_horse_hames = os.listdir(validation_horse_dir)\n",
    "print(f'VAL SET HORSES: {validation_horse_hames[:10]}')\n",
    "\n",
    "validation_human_names = os.listdir(validation_human_dir)\n",
    "print(f'VAL SET HUMANS: {validation_human_names[:10]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c334edcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training horse images: 500\n",
      "total training human images: 527\n",
      "total validation horse images: 128\n",
      "total validation human images: 128\n"
     ]
    }
   ],
   "source": [
    "print(f'total training horse images: {len(os.listdir(train_horse_dir))}')\n",
    "print(f'total training human images: {len(os.listdir(train_human_dir))}')\n",
    "print(f'total validation horse images: {len(os.listdir(validation_horse_dir))}')\n",
    "print(f'total validation human images: {len(os.listdir(validation_human_dir))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83013c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-15 19:20:02.551834: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-15 19:20:02.551873: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-06-15 19:20:06.070634: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-06-15 19:20:06.070698: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-06-15 19:20:06.070745: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (fadilrisdian-X455LAB): /proc/driver/nvidia/version does not exist\n",
      "2022-06-15 19:20:06.071151: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    # Note the input shape is the desired size of the image 150x150 with 3 bytes color\n",
    "    # This is the first convolution\n",
    "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    # The second convolution\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # The third convolution\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "#     # The fourth convolution (You can uncomment the 4th and 5th conv layers later to see the effect)\n",
    "#     tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "#     tf.keras.layers.MaxPooling2D(2,2),\n",
    "#     # The fifth convolution\n",
    "#     tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "#     tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # Flatten the results to feed into a DNN\n",
    "    tf.keras.layers.Flatten(),\n",
    "    # 512 neuron hidden layer\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('horses') and 1 for the other ('humans')\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d68961c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=RMSprop(learning_rate=0.001),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "479aa65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1027 images belonging to 2 classes.\n",
      "Found 256 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# All images will be rescaled by 1./255\n",
    "train_datagen = ImageDataGenerator(rescale=1/255)\n",
    "validation_datagen = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "# Flow training images in batches of 128 using train_datagen generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        './horse-or-human/',  # This is the source directory for training images\n",
    "        target_size=(150, 150),  # All images will be resized to 150x150\n",
    "        batch_size=128,\n",
    "        # Since you used binary_crossentropy loss, you need binary labels\n",
    "        class_mode='binary')\n",
    "\n",
    "# Flow training images in batches of 128 using train_datagen generator\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        './validation-horse-or-human/',  # This is the source directory for training images\n",
    "        target_size=(150, 150),  # All images will be resized to 150x150\n",
    "        batch_size=32,\n",
    "        # Since you used binary_crossentropy loss, you need binary labels\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef5f6d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "8/8 [==============================] - 28s 3s/step - loss: 1.2748 - accuracy: 0.5328 - val_loss: 0.7571 - val_accuracy: 0.5000\n",
      "Epoch 2/15\n",
      "8/8 [==============================] - 26s 3s/step - loss: 0.7241 - accuracy: 0.7241 - val_loss: 2.0349 - val_accuracy: 0.5000\n",
      "Epoch 3/15\n",
      "8/8 [==============================] - 24s 3s/step - loss: 0.6284 - accuracy: 0.7531 - val_loss: 1.0552 - val_accuracy: 0.5469\n",
      "Epoch 4/15\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.2778 - accuracy: 0.8587 - val_loss: 1.1375 - val_accuracy: 0.7578\n",
      "Epoch 5/15\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.1960 - accuracy: 0.9210 - val_loss: 1.1535 - val_accuracy: 0.7461\n",
      "Epoch 6/15\n",
      "8/8 [==============================] - 21s 3s/step - loss: 0.0504 - accuracy: 0.9833 - val_loss: 2.3097 - val_accuracy: 0.6875\n",
      "Epoch 7/15\n",
      "8/8 [==============================] - 21s 3s/step - loss: 0.9182 - accuracy: 0.8521 - val_loss: 1.0644 - val_accuracy: 0.7500\n",
      "Epoch 8/15\n",
      "8/8 [==============================] - 21s 2s/step - loss: 0.0584 - accuracy: 0.9811 - val_loss: 1.3785 - val_accuracy: 0.7773\n",
      "Epoch 9/15\n",
      "8/8 [==============================] - 22s 3s/step - loss: 0.0322 - accuracy: 0.9893 - val_loss: 1.5694 - val_accuracy: 0.7852\n",
      "Epoch 10/15\n",
      "8/8 [==============================] - 21s 3s/step - loss: 0.0153 - accuracy: 0.9967 - val_loss: 1.4160 - val_accuracy: 0.8242\n",
      "Epoch 11/15\n",
      "8/8 [==============================] - 20s 2s/step - loss: 0.0228 - accuracy: 0.9933 - val_loss: 1.6671 - val_accuracy: 0.7969\n",
      "Epoch 12/15\n",
      "8/8 [==============================] - 20s 3s/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 2.1596 - val_accuracy: 0.7852\n",
      "Epoch 13/15\n",
      "8/8 [==============================] - 21s 3s/step - loss: 1.9481 - accuracy: 0.8765 - val_loss: 0.6722 - val_accuracy: 0.8242\n",
      "Epoch 14/15\n",
      "8/8 [==============================] - 21s 2s/step - loss: 0.0514 - accuracy: 0.9933 - val_loss: 0.7794 - val_accuracy: 0.8438\n",
      "Epoch 15/15\n",
      "8/8 [==============================] - 21s 3s/step - loss: 0.0184 - accuracy: 0.9989 - val_loss: 1.2930 - val_accuracy: 0.7930\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "      train_generator,\n",
    "      steps_per_epoch=8,  \n",
    "      epochs=15,\n",
    "      verbose=1,\n",
    "      validation_data = validation_generator,\n",
    "      validation_steps=8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
